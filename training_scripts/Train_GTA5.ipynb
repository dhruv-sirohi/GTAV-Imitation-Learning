{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_GTA5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28fr9Fy1k8yH"
      },
      "source": [
        "Note: must turn off radar in settings before recording"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ofl8tG_xXM0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba12beb0-6351-43f9-dab7-767556773c3b"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#!unzip -u \"/content/drive/My Drive/Archive.zip\" -d \"/content/drive/My Drive/\"\n",
        "\n",
        "#basically, save it to a 'colab data' folder\n",
        "#then upload each file onto colab"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8BmnJ5Yzjyb"
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/GTA Driving Data\")\n",
        "#!ls"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNkuehuqjPe9"
      },
      "source": [
        "#imports + file upload\n",
        "%tensorflow_version 2.x\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense,Activation,Dropout,Flatten,Conv2D,MaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "assert len(tf.config.list_physical_devices('GPU')) > 0\n",
        "\n",
        "from google.colab import files\n",
        "from skimage.color import rgb2gray\n",
        "from collections import Counter\n",
        "from random import shuffle\n",
        "import cv2\n",
        "from IPython.display import Image, display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#uploaded = files.upload()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lu6X4MSkTQL"
      },
      "source": [
        "#data processing\n",
        "\n",
        "#next steps: use GAN to debias dataset (MIT 6.S191 lec 4, lab 2)\n",
        "\n",
        "def data_processing(start_val,num_files):\n",
        "  s_turn = []\n",
        "  m_turn = []\n",
        "  adj_turn = []\n",
        "  no_turn = []\n",
        "  balanced_data = []\n",
        "  starting_value = start_val\n",
        "  continue_loop = True\n",
        "  #start_time = time.time()\n",
        "  while continue_loop:\n",
        "    data_file_name = '/content/drive/MyDrive/GTA Driving Data/processed-training_data-{}.npy'.format(starting_value)\n",
        "    if os.path.isfile(data_file_name) and starting_value < start_val + num_files:\n",
        "      print(starting_value)\n",
        "      starting_value += 1\n",
        "      processed_data = np.load(data_file_name,allow_pickle=True)    \n",
        "      #print(len(processed_data))\n",
        "      #h+= 1\n",
        "      #i = 0\n",
        "      for entry in processed_data:\n",
        "        #print(i)\n",
        "        #i += 1\n",
        "        #np_entry =print(entry[0])\n",
        "        y = entry[0][0:3]\n",
        "        y_ls = y[2]\n",
        "        x= entry[1]\n",
        "        #x = rgb2gray(entry[1])\n",
        "        #flipping images\n",
        "        y_flipped = [y[0],y[1],1-y_ls]\n",
        "        x_flipped = cv2.flip(x, 1)\n",
        "        #print(y_ls)\n",
        "        if (y_ls > 0.875 or y_ls < 0.125):\n",
        "          #very sharp turn, but it doesn't warrant its own array\n",
        "          s_turn.append([x,y])\n",
        "          #s_turn.append([x_flipped,y_flipped])\n",
        "        elif (y_ls > 0.8 or y_ls < 0.2):\n",
        "          #sharp turn\n",
        "          #s_turn.append([x_flipped,y_flipped])\n",
        "          s_turn.append([x,y])\n",
        "        elif (y_ls > 0.7 or y_ls < 0.3):\n",
        "          #medium turn\n",
        "          m_turn.append([x,y])\n",
        "          #m_turn.append([x_flipped,y_flipped])\n",
        "\n",
        "        elif (y_ls > 0.65 or y_ls < 0.35):\n",
        "          #light turn, doesn't warrant its own array\n",
        "          m_turn.append([x,y])\n",
        "          #m_turn.append([x_flipped,y_flipped])\n",
        "\n",
        "        elif (y_ls > 0.575 or y_ls < 0.425):\n",
        "          #adjustments\n",
        "          adj_turn.append([x,y])\n",
        "          #adj_turn.append([x_flipped,y_flipped])\n",
        "        else:\n",
        "          #no turn\n",
        "          no_turn.append([x,y])\n",
        "          #no_turn.append([x_flipped,y_flipped])\n",
        "    else:\n",
        "      if not (starting_value < start_val + num_files):\n",
        "        continue_loop = False\n",
        "      else:\n",
        "        starting_value += 1\n",
        "\n",
        "  shuffle(no_turn)\n",
        "  #without shuffling, array adjustment in next step would mean that the training set is full of mirror repeats \n",
        "  #we would prefer to have [img12_mirrored,img1] rather than [img1_mirrored, img1]\n",
        "  #shuffling allows for this\n",
        "\n",
        "  \"\"\"\n",
        "  print(len(no_turn))\n",
        "  print(len(s_turn))\n",
        "  print(len(m_turn))\n",
        "  print(len(adj_turn))\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  arr_lengths = [len(no_turn),len(adj_turn),len(m_turn),len(s_turn)]\n",
        "  arr_names = [\"no turn\",\"adjustment turn\",\"medium turn\", \"sharp turn\"]\n",
        "  print(\"set up bar graph for adjusted list\")\n",
        "  h+= 1\n",
        "  \"\"\"\n",
        "  balanced_data = s_turn + m_turn + adj_turn + no_turn[0:len(adj_turn)]\n",
        "\n",
        "  #arr_lengths = [len(adj_turn),len(adj_turn),len(m_turn),len(s_turn)]\n",
        "  #arr_names = [\"no turn (trimmed)\",\"adjustment turn\",\"medium turn\", \"sharp turn\"]\n",
        "  \n",
        "  #plt.bar(arr_names,arr_lengths)\n",
        "  #plt.show()\n",
        "  \n",
        "\n",
        "  no_turn = []\n",
        "  s_turn = []\n",
        "  m_turn = []\n",
        "  adj_turn = []\n",
        "  shuffle(balanced_data)\n",
        "  X = []\n",
        "  Y = []\n",
        "  for entry in balanced_data:\n",
        "    entry[0] = rgb2gray(entry[0])\n",
        "\n",
        "\n",
        "  X , Y = zip(*balanced_data)\n",
        "\n",
        "  print(\"done! have {} test cases\".format(len(balanced_data)))\n",
        "  #print(\"time to run time: {}\".format(time.time()-start_time))\n",
        "  balanced_data = []\n",
        "\n",
        "  X_train_3dim = tf.convert_to_tensor(X)\n",
        "  #Y = tf.convert_to_tensor(Y)\n",
        "\n",
        "  #print(X_train_3dim.shape)\n",
        "\n",
        "  new_shape = list(X_train_3dim.shape) + [1]\n",
        "\n",
        "  X_train = tf.reshape(X_train_3dim,new_shape)\n",
        "  #print(X_train.shape)\n",
        "\n",
        "  Y_train_3dim = tf.convert_to_tensor(Y)\n",
        "  #Y = tf.convert_to_tensor(Y)\n",
        "\n",
        "  #print(Y_train_3dim.shape)\n",
        "\n",
        "  new_shape = list(Y_train_3dim.shape) + [1]\n",
        "\n",
        "  Y_train = tf.transpose(tf.reshape(Y_train_3dim,new_shape),perm = [0,2,1])\n",
        "  #print(Y_train.shape)\n",
        "\n",
        "  #we want agent to make small adjustments, so adj_turn isn't being adjusted\n",
        "  #we don't want agent to just go straight, so less of no_turn is being added\n",
        "  return X_train,Y_train"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SzgDspAwdbu"
      },
      "source": [
        "#saving chunk of training data as testing data\n",
        "\n",
        "#PERCENT_TEST = 20\n",
        "\n",
        "#test_index_start = int(PERCENT_TEST*len(balanced_data)/100)\n",
        "#train_data = balanced_data[:-test_index_start]\n",
        "#test_data = balanced_data[-test_index_start:]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoPO2OYnqc9K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "100ca80a-04ba-49ae-89cf-f919460ca9c4"
      },
      "source": [
        "#X_train = []\n",
        "#Y_train = []\n",
        "\n",
        "#for entry in train_data:\n",
        "  #print(entry)\n",
        "#  X_train.append(entry[0])\n",
        "#  Y_train.append(entry[1])\n",
        "\n",
        "#X_train = np.array(X_train)\n",
        "#Y_train = np.array(Y_train)\n",
        "\"\"\"\n",
        "\n",
        "X_train_3dim = tf.convert_to_tensor(X)\n",
        "#Y = tf.convert_to_tensor(Y)\n",
        "\n",
        "print(X_train_3dim.shape)\n",
        "\n",
        "new_shape = list(X_train_3dim.shape) + [1]\n",
        "\n",
        "X_train = tf.reshape(X_train_3dim,new_shape)\n",
        "print(X_train.shape)\n",
        "\n",
        "Y_train_3dim = tf.convert_to_tensor(Y)\n",
        "#Y = tf.convert_to_tensor(Y)\n",
        "\n",
        "print(Y_train_3dim.shape)\n",
        "\n",
        "new_shape = list(Y_train_3dim.shape) + [1]\n",
        "\n",
        "Y_train = tf.transpose(tf.reshape(Y_train_3dim,new_shape),perm = [0,2,1])\n",
        "print(Y_train.shape)\n",
        "\"\"\"\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\nX_train_3dim = tf.convert_to_tensor(X)\\n#Y = tf.convert_to_tensor(Y)\\n\\nprint(X_train_3dim.shape)\\n\\nnew_shape = list(X_train_3dim.shape) + [1]\\n\\nX_train = tf.reshape(X_train_3dim,new_shape)\\nprint(X_train.shape)\\n\\nY_train_3dim = tf.convert_to_tensor(Y)\\n#Y = tf.convert_to_tensor(Y)\\n\\nprint(Y_train_3dim.shape)\\n\\nnew_shape = list(Y_train_3dim.shape) + [1]\\n\\nY_train = tf.transpose(tf.reshape(Y_train_3dim,new_shape),perm = [0,2,1])\\nprint(Y_train.shape)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "G0io4xZpQk_X",
        "outputId": "1c7e5e56-9d8d-48e2-8b80-f734f120867b"
      },
      "source": [
        "#new_shape_0 = [1] + list(X_train[0].shape)\n",
        "#X_train_0= tf.reshape(X_train[0],new_shape_0)\n",
        "\"\"\"\n",
        "print(X_train[0:10].shape)\n",
        "\"\"\"\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nprint(X_train[0:10].shape)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "d-ou3jsRRkRo",
        "outputId": "3882f4c5-d4d5-47cf-f2ad-7f5c635ef9c2"
      },
      "source": [
        "\"\"\"\n",
        "print(Y_train.shape)\n",
        "print(tf.transpose(Y_train,perm = [0,2,1]).shape)\n",
        "\"\"\"\n",
        "#new_shape_0 = [1] + list(X_train[0].shape)\n",
        "#X_train_0 = tf.reshape(X_train[0],new_shape_0)\n",
        "#print(X_train_0.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nprint(Y_train.shape)\\nprint(tf.transpose(Y_train,perm = [0,2,1]).shape)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwYKIJtKtsPl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775
        },
        "outputId": "ba73ec0a-dbe7-48fe-c068-0e11df27a26f"
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "import mxnet as mx\n",
        "from mxnet import np, npx\n",
        "from mxnet.gluon import nn\n",
        "from d2l import mxnet as d2l\n",
        "\"\"\"\n",
        "\n",
        "#set up alexnet\n",
        "\n",
        "\n",
        "def build_alexnet_model():\n",
        "  #image_shape = (80,60,1)\n",
        "  np.random.seed(1000)\n",
        "  #https://www.tensorflow.org/api_docs/python/tf/keras/layers/InputLayer\n",
        "  model = tf.keras.Sequential([\n",
        "    #tf.keras.layers.InputLayer(input_shape=image_shape),\n",
        "    tf.keras.layers.Conv2D(filters=96, kernel_size=(5,5),strides=(2,2),activation=tf.nn.relu), \n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(3,3),strides=(2,2)),\n",
        "    tf.keras.layers.Conv2D(filters=256,kernel_size=(5,5),strides=(1,1),activation=tf.nn.relu), \n",
        "    #tf.keras.layers.MaxPooling2D(pool_size=(3,3),strides=(2,2)),\n",
        "    #tf.keras.layers.Conv2D(filters=384,kernel_size=(3,3),strides=(1,1),activation=tf.nn.relu), \n",
        "    tf.keras.layers.Conv2D(filters=384,kernel_size=(3,3),strides=(1,1),activation=tf.nn.relu), \n",
        "    tf.keras.layers.Conv2D(filters=256,kernel_size=(3,3),strides=(1,1),activation=tf.nn.relu), \n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(3,3),strides=(2,2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(4096, activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dropout(0.4),\n",
        "    tf.keras.layers.Dense(4096, activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dropout(0.4),\n",
        "    tf.keras.layers.Dense(3, activation=tf.keras.activations.linear)])\n",
        "  \n",
        "  return model\n",
        "\n",
        "def build_cnn_model():\n",
        "      cnn_model = tf.keras.Sequential([\n",
        "        # Here, we use a larger 11 x 11 window to capture objects. At the same\n",
        "        # time, we use a stride of 2 to reduce the height and width of\n",
        "        # the output. Here, the number of output channels is much larger than\n",
        "        # that in LeNet\n",
        "        tf.keras.layers.Conv2D(filters=96, kernel_size=11, strides=2,\n",
        "                               activation='relu'),\n",
        "        tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\n",
        "        # Make the convolution window smaller, set padding to 2 for consistent\n",
        "        # height and width across the input and output, and increase the\n",
        "        # number of output channels\n",
        "        tf.keras.layers.Conv2D(filters=256, kernel_size=5, padding='same',\n",
        "                               activation='relu'),\n",
        "        tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\n",
        "        # Use three successive convolutional layers and a smaller convolution\n",
        "        # window. Except for the final convolutional layer, the number of\n",
        "        # output channels is further increased. Pooling layers are not used to\n",
        "        # reduce the height and width of input after the first two\n",
        "        # convolutional layers\n",
        "        tf.keras.layers.Conv2D(filters=384, kernel_size=3, padding='same',\n",
        "                               activation='relu'),\n",
        "        tf.keras.layers.Conv2D(filters=384, kernel_size=3, padding='same',\n",
        "                               activation='relu'),\n",
        "        tf.keras.layers.Conv2D(filters=256, kernel_size=3, padding='same',\n",
        "                               activation='relu'),\n",
        "        tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        # Here, the number of outputs of the fully-connected layer is several\n",
        "        # times larger than that in LeNet. Use the dropout layer to mitigate\n",
        "        # overfitting\n",
        "        tf.keras.layers.Dense(4096, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(4096, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        # Output layer. Since we are using Fashion-MNIST, the number of\n",
        "        # classes is 10, instead of 1000 as in the paper\n",
        "        tf.keras.layers.Dense(3)])\n",
        "      \n",
        "      return cnn_model\n",
        "\n",
        "cnn_model = build_cnn_model()\n",
        "cnn_model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1e-1), \n",
        "              loss=tf.keras.losses.MeanSquaredError(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#x = np.random.rand(2,160,120,1)\n",
        "#X = tf.convert_to_tensor(x)\n",
        "\n",
        "# Initialize the model by passing some data through\n",
        "#predictions = cnn_model.predict(X_train_0)\n",
        "#print(predictions)\n",
        "#print(predictions.shape)\n",
        "# Print the summary of the layers in the model.\n",
        "#print(cnn_model.summary())\n",
        "\n",
        "checkpoint_path = \"/content/drive/MyDrive/GTA Driving Data/training_1/cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)\n",
        "\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 3\n",
        "\n",
        "time_vals = []\n",
        "train_vals = []\n",
        "test_vals = []\n",
        "files_in_epoch = 0\n",
        "accuracy_store = 0\n",
        "multiplier = 1\n",
        "start_val = 0\n",
        "num_files_to_retrieve = 1\n",
        "num_files_to_test = 2\n",
        "for epoch in range(0,EPOCHS):\n",
        "  print('-------------STARTING EPOCH {}-------------'.format(epoch))\n",
        "  while start_val < num_files_to_test-num_files_to_retrieve*3:\n",
        "    if(start_val < 83):\n",
        "      X_train, Y_train = data_processing(start_val,num_files_to_retrieve)\n",
        "      #print(\"len {}\".format(len(X_train)))\n",
        "      start_val += num_files_to_retrieve\n",
        "      multiplier = num_files_to_retrieve\n",
        "    else:\n",
        "      X_train, Y_train = data_processing(start_val,num_files_to_retrieve*3)\n",
        "      start_val += num_files_to_retrieve*3\n",
        "      multiplier = num_files_to_retrieve*3\n",
        "    \n",
        "    num_files = X_train.shape[0]\n",
        "    files_in_epoch += num_files\n",
        "\n",
        "    history = cnn_model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=1)#,callbacks=[cp_callback])\n",
        "    \n",
        "    group_accuracy = num_files*history.history['accuracy'][0]\n",
        "    print(\"group accuracy: {}\".format(group_accuracy))\n",
        "    accuracy_store += group_accuracy\n",
        "    \n",
        "\n",
        "  X_train, Y_train = data_processing(start_val,num_files_to_test-start_val+1)\n",
        "  #print(\"len {}\".format(len(X_train)))\n",
        "\n",
        "  num_files = X_train.shape[0]\n",
        "  files_in_epoch += num_files\n",
        "\n",
        "  history = cnn_model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=1)#,callbacks=[cp_callback])\n",
        "  \n",
        "  multiplier = num_files_to_test-start_val+1\n",
        "  group_accuracy = num_files*history.history['accuracy'][0]\n",
        "  accuracy_store += group_accuracy\n",
        "\n",
        "  train_vals.append(accuracy_store/(files_in_epoch))\n",
        "  files_in_epoch = 0\n",
        "  accuracy_store = 0\n",
        "  #print(accuracy_vals)\n",
        "  time_vals.append(epoch+1)\n",
        "\n",
        "  X_test, Y_test = data_processing(201,1)#33)\n",
        "  test_loss, test_acc = cnn_model.evaluate(X_test,Y_test)\n",
        "  print('Test accuracy:', test_acc)\n",
        "  test_vals.append(test_acc)\n",
        "\n",
        "  #saving model\n",
        "  if epoch%5 == 0 and epoch != 0:\n",
        "    file_name = \"/content/drive/MyDrive/GTA Driving Data/training weights/gta_cnn_model_epoch_{}.h5\".format(epoch+1)\n",
        "    cnn_model.save(file_name)\n",
        "\n",
        "  start_val = 0\n",
        "\n",
        "plt.plot(time_vals, test_vals, label = \"test accuracy\")\n",
        "plt.plot(time_vals, train_vals, label = \"train accuracy\")\n",
        "plt.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------STARTING EPOCH 0-------------\n",
            "0\n",
            "1\n",
            "2\n",
            "done! have 3267 test cases\n",
            "103/103 [==============================] - 6s 52ms/step - loss: 0.0360 - accuracy: 0.8256\n",
            "201\n",
            "done! have 368 test cases\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.0704 - accuracy: 0.5111\n",
            "Test accuracy: 0.5111111402511597\n",
            "-------------STARTING EPOCH 1-------------\n",
            "0\n",
            "1\n",
            "2\n",
            "done! have 3267 test cases\n",
            "103/103 [==============================] - 5s 53ms/step - loss: 0.0284 - accuracy: 0.8296\n",
            "201\n",
            "done! have 368 test cases\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.0536 - accuracy: 0.5000\n",
            "Test accuracy: 0.5\n",
            "-------------STARTING EPOCH 2-------------\n",
            "0\n",
            "1\n",
            "2\n",
            "done! have 3267 test cases\n",
            "103/103 [==============================] - 6s 53ms/step - loss: 0.0281 - accuracy: 0.8318\n",
            "201\n",
            "done! have 368 test cases\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0486 - accuracy: 0.5083\n",
            "Test accuracy: 0.5083333253860474\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOoguOvCdOGQ"
      },
      "source": [
        "\"\"\"\n",
        "#defining lstm model\n",
        "#rnn is later\n",
        "def build_LSTM_model():#vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = keras.Sequential(\n",
        "      [\n",
        "       layers.ConvLSTM2D(\n",
        "           filters=40,kernel_size=(3,3),padding='same',return_sequences=True\n",
        "       ),\n",
        "       layers.BatchNormalization(),\n",
        "       layers.ConvLSTM2D(\n",
        "           filters=40,kernel_size=(3,3),padding='same',return_sequences=True\n",
        "       ),\n",
        "       layers.BatchNormalization()\n",
        "       ])\n",
        "  model.compile(loss=keras.losses.MeanSquaredError(),optimizer='adam',metrics=[\"accuracy\"])\n",
        "  return model\n",
        "\n",
        "x = np.random.rand(1600,120,160,1)\n",
        "X = tf.convert_to_tensor(x)\n",
        "print(X.shape[0])\n",
        "\n",
        "\n",
        "rnn_model = build_LSTM_model()\n",
        "# Initialize the model by passing some data through\n",
        "predictions = rnn_model.predict(X)\n",
        "\n",
        "#print(predictions)\n",
        "# Print the summary of the layers in the model.\n",
        "print(rnn_model.summary())\n",
        "\n",
        "\n",
        "\n",
        "    # Layer 1: Embedding layer to transform indices into dense vectors \n",
        "    #   of a fixed embedding size\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "\n",
        "    # Layer 2: LSTM with `rnn_units` number of units. \n",
        "    # TODO: Call the LSTM function defined above to add this layer.\n",
        "    LSTM(rnn_units),\n",
        "\n",
        "    # Layer 3: Dense (fully-connected) layer that transforms the LSTM output\n",
        "    #   into the vocabulary size. \n",
        "    # TODO: Add the Dense layer.\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "\"\"\"\n",
        "\n",
        "  #return model\n",
        "\n",
        "# Build a simple model with default hyperparameters. You will get the \n",
        "#   chance to change these later.\n",
        "#model = build_model(len(vocab), embedding_dim=256, rnn_units=1024, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cINM9ewdY2V"
      },
      "source": [
        "#figure out batches w/ input data, set that up, break them down into 100 frames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPq2o3i0u3LT"
      },
      "source": [
        "\"\"\"\n",
        "# saving and loading the model weights\n",
        " \n",
        "# save model\n",
        "model.save_weights('gfgModelWeights')\n",
        "print('Model Saved!')\n",
        " \n",
        "# load model\n",
        "savedModel = model.load_weights('gfgModelWeights')\n",
        "print('Model Loaded!')\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkQnuYsPgrlO"
      },
      "source": [
        ""
      ]
    }
  ]
}